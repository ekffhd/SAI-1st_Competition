A Data Science Framework To Achieve 99% Accuracy
 - 발표자 : 서기원
 - 발표 일자 : 2019. 05. 15
 - Original address : https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy
 - 내용 :


1. How a Data Scientist Beat the Odds
 - 컴페티션 결과 : 바이너리 예측(발생 or 발생하지 않음)


2. A Data Science Framework
 (1) 문제 정의
  - 알고리즘 등의 설계를 시작하기 전, 문제 정의를 해야 한다.

 (2) 데이터 수집

 (3) 데이터 가공
  - wild/managable한 데이터를 가공한다.
  - 데이터 아키텍쳐 구성, 데이터 추출, 데이터 정리(누락 및 이상 값 식별) 등의 과정

 (4) 탐색적 분석
  - 데이터들 사이의 패턴, 분류, 상관관계 등을 비교하기 위해 그래프를 그려 데이터를 탐색해 보는 것이 좋다.

 (5) 모델 데이터
  - 데이터 세트와 prediction을 생각하며 잘 분석하여 알고리즘을 선정한다.
  - 학습시킬 모델을 잘 선정하는 것이 중요하다!

 (6) 모델 구현 및 정확도 검사
  - 모델을 구현하여 학습시킨 후, 정확도를 검사한다.
  - Overfit / Generalize / Underfit 하는지 확인한다.

 (7) 최적화 및 Strategize
  - 모델을 최적화 시킨다!
  - 파라미터 조정 등의 미세조정 과정이다.


3. Step
 (1) Step 1 : Define the Problem
  - 문제 정의 : 이 프로젝트의 문제는 타이타닉 호의 승객 생존 결과를 예측하는 알고리즘을 개발하는 것이다.
  - 프로젝트 요약 : 타이타닉의 침몰은 역사상 가장 악명 높은 난파선 중 하나이다. 1912년 4월 15일 타이타닉은 빙산과 충돌한 후 침몰하여 2224명의 승객과
승무원 중 1502 명이 사망했다. 난파선으로 인해 많은 사람이 죽은 이유 중 하나는 승객과 승무원을 위한 구명정이 충분하지 않았기 때문이다. 침몰에서 살아남는
데는 몇 가지 행운이 있었지만 여성, 어린이 및 상류층과 같은 일부 그룹은 다른 사람들보다 생존 가능성이 더 컸다. 이러한 과제에서 우리는 어떤 종류의 사람들이
생존 할 가능성이 있는지에 대해 분석해야 한다! 기계학습 도구를 적용하여 해당 승객이 이 비극에서 살아남을지를 예측하자.
  - 기술 : 이진 분류, 파이썬/R

 (2) Step 2 : Gather the Data
  - 캐글에서 트레인/테스트 데이터 셋을 준다!

 (3) Step 3 : Prepare Data for Consumption
  1-1) Import Libraries
   - 코드는 Python 3.x로 작성 되었다.
   - 라이브러리는 필요한 작업을 수행하기 위해 미리 작성된 기능을 제공한다.
import sys #system parameters로 접근한다.
import pandas as pd #기계학습에 도움되는 판다스
import matplotlib #시각화에 도움됨
import numpy as np #수학적 계산에 도움 됨
import scipy as sp #계산에 필요한 함수들을 가지고 있음
import IPython
from IPython import display #data frame을 jupyter notebook에서 잘 출력하기 위함
import sklearn #많은 기계학습 알고리즘을 가지고 있음
import random
import time

#warnings 무시
import warnings
warnings.filterwarnings('ignore')
print('-'*25)

from subprocess import check_output
print(check_output(["Is", "../input"]).decode("utf8")) #data set들 확인하기. 캐글 커널상에서 돌릴 경우, ../input/을 통해 input data를 이용할 수 있다.

  1-2) Load Data Modelling Libraries
   - 기본적으로 scikit-learn 라이브러리를 사용하여 기계학습 알고리즘을 이용할 것이다.
   - 데이터 시각화를 위해 matplotlib와 seaborn 라이브러리를 사용한다.
#일반적인 Model Algorithms
from sklearn import svm, tree, linear_model, neighbors, naive_bays, ensemble, discriminant_analysis, gaussian_process
from xgboost import XGBClassifier

#일반적인 Model 도우미들
from sklearn.preprocessing import OneHotEncoder, LabelEncoder #데이터 전처리
from sklearn import feature_selection #특징 선택
from sklearn import model_selection #모델 선택
from sklearn import metrics #평가

#Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns #seaborn 시각화
from pandas.tools.plotting import scatter_matrix #산점도

#Visualization 이용시 필수 함수
%matplotlib inline #주피터 노트북에서 mlt를 볼 수 있게 해줌
mpl.style.use('ggplot') #mlt 스타일 지정
sns.set_style('white') #seaborn 스타일 지정
pylab.rcParms['figure.gifsize'] = 12, 8 #figsize

  2-1) Meet and Greet Data
   - 데이터 유형/값, 변수 등 데이터를 탐색해 본다.
   - info()와 sample() 함수로 데이터 유형의 간결한 개요를 볼 수 있다.
   - Survived : 생존한 경우 1, 생존 못 한 경우 0
     PassengerID / Ticket : 결과 변수에 영향을 주지 않고 '대상을 식별하기 위한 것'으로 가정한다. 분석에서 제외된다.
     Pclass : 상위 클래스는 1, 중간 클래스는 2, 하위 클래스는 3이다.
     Name : 공칭 데이터 유형. 성의 성별 및 의사/석사 등을 구분할 수 있게 도와준다.
     Sex / Embarked : 공칭 데이터 유형. 수학 계산을 위해 더미 변수로 변환되어야 한다.
     Age / Fare : 양적 데이터 변수이다.
     SibSP / Parch : SibSP는 관련된 형제 / 배우자의 수를 나타내며, Parch는 관련된 부모 / 자녀의 수를 나타낸다. 개별의 정량 데이터 유형이다.
     Cabin : 사고가 발생했을 때 선박의 대략적인 위치와 갑판 수준의 SES에서 엔지니어링에 사용할 수 있는 공칭 데이터 유형이다. null이 많으므로 분석에서 제외된다.
data_raw = pd.read_csv('../input/train.csv') #판다스를 이용해 input인 train 데이터의 csv파일을 읽는다. train을 train/test로 쪼개고 또 각각 input/output 피쳐를 구분해서 이용할 예정!
data_val = pd.read_csv('../input/test.csv') #test에 대한 output을 최종적으로 예측해야 한다!
data1 = data.raw.copy(deep = True) #데이터를 잘 이용하기 위해 복제!
data_cleaner = [data1, data_val] #cleaner를 이용해서 간단하게 데이터 한 번 정제하기.

print(data_raw.info()) #data_raw 데이터에 대한 information을 간단하게 확인한다.
data_raw.sample(10)

  2-2) The 4 C's Data Cleaning : Correcting, Completing, Creating, and Converting
   - 이 단계에서는 비정상적 값과 특이 치를 수정하고, 누락 정보를 완성하며, 분석을 위해 새 기능을 만들고, 계산 및 표현을 위해 형식을 변환한다.
   - Correcting : 비정상적의 데이터를 수정한다. 비정상적의 예는 80세가 아닌 800세인 경우이다.
     Completing : Null / Nan 값 등 값이 누락 된 경우 모델이 처리 못할 수 있다. 이 문제 해결하기! Nan을 삭제하거나 0, 평균 값 취하기 등 다양한 방법이 있다.
     Creating : 피쳐 별로 생존에 영향을 줬는지 title을 부여할 것이다.
     Converting : 다양한 데이터들을 계산 가능한 데이터로 변환한다!
print('Train columns with null values : \n', data1.isnull().sum()) #Train 데이터 중 null을 확인한다.
print("-"*10)

print('Test/Validation columns with null vaues : \n', data_val.isnull().sum()) #Test/Validation 데이터 중 null을 확인한다.
print("-"*10)

data_raw.describe(include = 'all') #data_raw의 데이터를 확인한다.

  2-3) Clean Data
#####Completing#####
for dataset in data_cleaner : #data_cleaner 함수 지정
	dataset['Age'].fillna(dataset['Age'].median(), inplace = True) #Age 중 빈 영역을 평균값으로 채운다.
	dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True) #Embarked 중 빈 부분을 mode로 채운다.
	dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True) #Fare 중 빈 부분을 평균값으로 채운다.

drop_column = ['PassengerId', 'Cabin', 'Ticket'] #결과에 영향을 주지 않는다고 판단되는 피쳐를 train set에서 제거한다.
data1.drop(drop_column, axis=1, inplace = True)

print(data1.isnull().sum()) #data1의 null 확인
print("-"*10)
print(data_val.isnull().sum()) #data_val의 null 확인
##########

#####Create#####
for dataset in dat_cleaner:
	dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 #형제/배우자 등을 포함해 Family Size라는 새로운 특징으로 만듦.
	dataset['IsAlone'] = 1 #혼자일 경우 1
	dataset['IsAlone'].loc[dataset['FamilySize']>1] = 0 #가족 구성원이 1 초과일 경우 0
	dataset['Title'] = dataset['Name'].str.split(", ", expand = True)[1].str.split(".", expand = True)[0] #',' 혹은 '.'으로 이름을 분리하고, Title(Miss/mr 등)을 얻는다.
	dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)
	dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)

	#Title 정제(Mr/Miss/Mrs/Master/Misc)
	stat_min = 10
	title_names = (data1['Title'].value_counts < stat_min) #10보다 작은 경우
	data1['Title'] = data1['Title'].apply(lambda x:"Misc" if title_names.loc[x] == True else x)
	print(data1['Title'].value_counts())
	print("-"*10)

	#데이터 확인
	data1.info()
	data_val.info()
	data1.sample(10)
##########

  2-4) Convert Formats
   - sklearn / pandas를 이용하여 범주형 데이터를 수학적으로 변환할 것이다.
   - 또한 데이터를 모델링에 이용하기 위해 x(input)와 y(output)로 나눌 것이다.
#####Convert#####
#수학적 계산을 위해 정제시킨 데이터들을 Encoder를 이용해 카테고리화 한다.
label = LabelEncoder()
for dataset in dat_cleaner :
	dataset['Sex_Code'] = label.fit_transform(datset['Sex'])
	dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])
	dataset['Title_Code'] = label.fit_transform(dataset['Title'])
	dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])
	dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])

# x(input)에 따른 y(output)을 정의한다.
Target = ['Survived']

# x(input)을 정의한다.
data1_x = ['Sex', Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #정제되기 전의 x
data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #정제되어 계산 가능한 x
data1_xy = Target + dadta1_x
print('Original X Y: ', data1_xy, '\n')

# binary 하게 만든 x
data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']
data1_xy_bin = Target + data1_x_bin
print('Bin X Y: ', data1_xy_bin, '\n')

# 더미 피처에 대한 x 및 y 변수 정의 original
data1_dummy = pd.get_dummies(data1[data1_x])
data1_x_dummy = data1_dummy.columns.tolist()
data1_xy_dummy = Target + data1_x_dummy
print('Dummy X Y: ', data1_xy_dummy, '\n')

data1_dummy.head()
##########

  2-5) Double Check Cleaned Data
   - 데이터를 전부 정제 했는지 더블 체크한다.
print('Train columns with null values: \n', data1.isnull().sum())
print("-"*10)
print (data1.info())
print("-"*10)

print('Test/Validation columns with null values: \n', data_val.isnull().sum())
print("-"*10)
print (data_val.info())
print("-"*10)

data_raw.describe(include = 'all')

  2-6) Split Training and Testing Data
   - Train set을 Train과 Test 데이터 셋으로 분리한다. 0.75 : 0.25의 비율로 분리한다.
   - sklearn의 train_test_split 함수를 사용한다.
train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)
train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)
train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)

print("Data1 Shape: {}".format(data1.shape))
print("Train1 Shape: {}".format(train1_x.shape))
print("Test1 Shape: {}".format(test1_x.shape))

train1_x_bin.head()

 (4) Step 4 : Perform Exploratory Anaysis with Statistics
  - 데이터 정리는 완료 되었다. 데이터를 탐색하기 위해 통계/그래프를 이용한다. 데이터의 기능을 분류하고 대상 변수 및 서로의 상관관계를 분석할 수 있다.
#각 피쳐인 Column들의 값의 평균과 Survival의 상관관계를 알아본다.
  1-1) 데이터 상관관계 확인
for x in data1_x:
    if data1[x].dtype != 'float64' :
        print('Survival Correlation by:', x)
        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean()) #Target(Survival)과 각 Column의 평균값을 groupby하여 상관관계를 알아본다.
        print('-'*10, '\n')

print(pd.crosstab(data1['Title'],data1[Target[0]]))

  1-2) 그래프로 분포 및 상관관계 확인
#양적 데이터의 Graph 분포
plt.figure(figsize=[16,12])

#Fare 분포
plt.subplot(231)
plt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)
plt.title('Fare Boxplot')
plt.ylabel('Fare ($)')

#Age 분포
plt.subplot(232)
plt.boxplot(data1['Age'], showmeans = True, meanline = True)
plt.title('Age Boxplot')
plt.ylabel('Age (Years)')

#FamilySize 분포
plt.subplot(233)
plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)
plt.title('Family Size Boxplot')
plt.ylabel('Family Size (#)')

#Fare과 Survived 상관관계
plt.subplot(234)
plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], 
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Fare Histogram by Survival')
plt.xlabel('Fare ($)')
plt.ylabel('# of Passengers')
plt.legend()

#Age와 Survived 상관관계
plt.subplot(235)
plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], 
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Age Histogram by Survival')
plt.xlabel('Age (Years)')
plt.ylabel('# of Passengers')
plt.legend()

#FimilySize와 Survived 상관관계
plt.subplot(236)
plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], 
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Family Size Histogram by Survival')
plt.xlabel('Family Size (#)')
plt.ylabel('# of Passengers')
plt.legend()

  1-3) Seaborn 그래프
fig, saxis = plt.subplots(2, 3,figsize=(16,12))

#barplot형 그래프
sns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0]) #Embarked와 Survived 상관관계
sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1]) #Pclass와 Survived 상관관계
sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2]) #IsAlone과 Survived 상관관계

#pointplot형 그래프
sns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0]) #FareBin과 Survived 상관관계
sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1]) #AgeBin과 Survived 상관관계
sns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2]) #FamilySize와 Survived 상관관계

#피쳐끼리의 상관관계 중요도 비교
fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))

#Pclass vs Fare
sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)
axis1.set_title('Pclass vs Fare Survival Comparison')

#Pclass vs Age
sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)
axis2.set_title('Pclass vs Age Survival Comparison')

#Pclass vs Family Size
sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)
axis3.set_title('Pclass vs Family Size Survival Comparison')

fig, qaxis = plt.subplots(1,3,figsize=(14,12))

#Sex vs Embarked
sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])
axis1.set_title('Sex vs Embarked Survival Comparison')

#Sex vs Pclass
sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])
axis1.set_title('Sex vs Pclass Survival Comparison')

#Sex vs IsAlone
sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])
axis1.set_title('Sex vs IsAlone Survival Comparison')

fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))

#family size vs sex in Survived
sns.pointplot(x="FamilySize", y="Survived", hue="Sex", data=data1,
              palette={"male": "blue", "female": "pink"},
              markers=["*", "o"], linestyles=["-", "--"], ax = maxis1)

#Pclass vs Sex in Survived
sns.pointplot(x="Pclass", y="Survived", hue="Sex", data=data1,
              palette={"male": "blue", "female": "pink"},
              markers=["*", "o"], linestyles=["-", "--"], ax = maxis2)

#Pclass vs Sex vs Embarked in Survived
e = sns.FacetGrid(data1, col = 'Embarked')
e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')
e.add_legend()

#Age에 따라 살아남는가, 살아남지 못하는가 확인
a = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )
a.map(sns.kdeplot, 'Age', shade= True )
a.set(xlim=(0 , data1['Age'].max()))
a.add_legend()

#Sex vs Class vs Age in Survived
h = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')
h.map(plt.hist, 'Age', alpha = .75)
h.add_legend()

#전체 데이터 셋의 pair 그래프
pp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )
pp.set(xticklabels=[])

  1-4) 히트맵
def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(14, 12))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)
    
    _ = sns.heatmap(
        df.corr(), 
        cmap = colormap,
        square=True, 
        cbar_kws={'shrink':.9 }, 
        ax=ax,
        annot=True, 
        linewidths=0.1,vmax=1.0, linecolor='white',
        annot_kws={'fontsize':12 }
    )
    
    plt.title('Pearson Correlation of Features', y=1.05, size=15)

correlation_heatmap(data1)

 (5) Step 5 : Model Data
  - 정답을 포함하는 Supervised Learning 방법을 이용한다.
  - 분류하는 알고리즘이 필요하다.
  - 문제에 맞는 최상의 알고리즘을 선택하는 방법은?
    그런 것은 없다! 여러 알고리즘을 시도하고, 조정하고, 비교해봐야 한다.
#머신러닝 알고리즘들!
MLA = [
    #Ensemble Methods
    ensemble.AdaBoostClassifier(),
    ensemble.BaggingClassifier(),
    ensemble.ExtraTreesClassifier(),
    ensemble.GradientBoostingClassifier(),
    ensemble.RandomForestClassifier(),

    #Gaussian Processes
    gaussian_process.GaussianProcessClassifier(),
    
    #GLM
    linear_model.LogisticRegressionCV(),
    linear_model.PassiveAggressiveClassifier(),
    linear_model.RidgeClassifierCV(),
    linear_model.SGDClassifier(),
    linear_model.Perceptron(),
    
    #Navies Bayes
    naive_bayes.BernoulliNB(),
    naive_bayes.GaussianNB(),
    
    #Nearest Neighbor
    neighbors.KNeighborsClassifier(),
    
    #SVM
    svm.SVC(probability=True),
    svm.NuSVC(probability=True),
    svm.LinearSVC(),
    
    #Trees    
    tree.DecisionTreeClassifier(),
    tree.ExtraTreeClassifier(),
    
    #Discriminant Analysis
    discriminant_analysis.LinearDiscriminantAnalysis(),
    discriminant_analysis.QuadraticDiscriminantAnalysis(),
    
    #xgboost
    XGBClassifier()    
    ]

#dataset을 cross-validation으로 분할하기.
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) #test : train = 3 : 6

#MLA 알고리즘들을 비교, 평가하기 위한 배열
MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']
MLA_compare = pd.DataFrame(columns = MLA_columns)

#MLA 알고리즘들의 predictions
MLA_predict = data1[Target]

#MLA의 수행 결과를 테이블 형식으로 저장하기
row_index = 0 #행
for alg in MLA:

    #name 및 파라미터 설정
    MLA_name = alg.__class__.__name__
    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name
    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())
    
    #cross validation을 통한 모델 평가
    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)

    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean() #학습 시간
    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() #학습 정확도
    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() #테스트 정확도
    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3 #일어날 수 있는 최악의 결과
    
    #MLA prediction 저장
    alg.fit(data1[data1_x_bin], data1[Target])
    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])
    
    row_index+=1 #for문 돌아가며 행 추가

MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)
MLA_compare #MLA 알고리즘들의 비교 테이블 출력!

#그래프를 통해서 각 알고리즘들의 성능 비교해 보기
sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')
plt.title('Machine Learning Algorithm Accuracy Score \n')
plt.xlabel('Accuracy Score (%)')
plt.ylabel('Algorithm')


4. Handmade 모델
 - 기본 MLA(기계 학습 알고리즘들)을 이용해서 82%의 정확도를 기록했다.
 - 작성자는 기존의 기계 학습 알고리즘 이외에 자신이 알고리즘을 구축해서 학습시켜봤다.
 - 지금까지 정리해보면 다음과 같다. 기계 학습 알고리즘을 생각할 때 다음의 조건을 생각하며 각 항목에서 Class 별로 생존/사망을 정하는 게임이라고 생각하면 좋다.
 - 남성 (81%) 사망 / 여성 (74%) 생존
   1등급 (97%) 생존 / 2등급 (92%) 생존
   S항 (63%) 사망
   
 (1) Group by / Pivot data
 pivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()
 print('Survival Decision Tree w/Female Node: \n',pivot_female)

 pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()
 print('\n\nSurvival Decision Tree w/Male Node: \n',pivot_male)
 
 (2) 작성자가 직접 구축한 Decision Tree 모델
  - 작성자는 직접 남성/여성, 1등급/2등급 등 기준별로 Alive/Dead를 결정해주는 Decision Tree를 작성했다.
 def mytree(df): #작성자가 직접 구축한 Decision Tree 모델
    
    Model = pd.DataFrame(data = {'Predict':[]}) #작성자의 모델 예측 결과를 저장하기 위해 테이블을 초기화함.
    male_title = ['Master'] #survived titles

    for index, row in df.iterrows():
    	#Question 1 : 당신이 타이타닉에 있었다면, 높은 확률로 죽었을 것이다.
        Model.loc[index, 'Predict'] = 0 #0. 죽음!

	#Question 2 : 당신이 여자라면, 높은 확률로 살았을 것이다.
        if (df.loc[index, 'Sex'] == 'female'): #여자라면
                  Model.loc[index, 'Predict'] = 1 #1. 삼!

	#Question 5B Female - FareBin; 여성 노드에서 0.5 미만의 값은 0으로 설정한다.
        if ((df.loc[index, 'Sex'] == 'female') & 
            (df.loc[index, 'Pclass'] == 3) & 
            (df.loc[index, 'Embarked'] == 'S')  &
            (df.loc[index, 'Fare'] > 8)
           ):
                  Model.loc[index, 'Predict'] = 0 #0. 죽음!
		  
	#Question 3B Male - Title; 남성 노드에서 0.5에서 1 사이의 값은 1로 설정한다.
        if ((df.loc[index, 'Sex'] == 'male') &
            (df.loc[index, 'Title'] in male_title)
            ):
            Model.loc[index, 'Predict'] = 1 #1. 삼!
                
    return Model
    
    #모델에 데이터를 넣어 예측값을 구한다.
    Tree_Predict = mytree(data1)
    #모델을 precision / recall / f1-score 등으로 평가해 본다.
    print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))

 (3) 모델 평가 및 시각화
  - Confusion matrix를 이용해서 모델을 평가해 보고, 다양한 시각화를 해 본다.
 import itertools
 def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    이 함수는 confusion matrix를 출력해볼 수 있게 해주고, plot을 그리게 해 준다.
    Normalization은 'normalize = True'를 통해서 이용할 수 있다.
    """
    if normalize: #Normalize를 이용할 경우
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else: #Normalize를 이용하지 않을 경우
        print('Confusion matrix, without normalization')

    print(cm)

    #시각화
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
    # Confusion matrix 계산
    cnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)
    np.set_printoptions(precision=2)

    class_names = ['Dead', 'Survived']
    
    # Normalized를 이용하지 않은 그래프 시각화
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names,
                      title='Confusion matrix, without normalization')

    # Normalized를 이용한 그래프 시각화
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, 
                      title='Normalized confusion matrix')
		      
		      
5. Cross Validation
 - Train data만을 이용하는 것이 아닌, Train data의 하위 집합을 사용해서 모델을 학습시키고 평가하는 것이 중요하다.
   그렇지 않으면 Overfitting 등의 문제가 있을 수 있다.
 - CV는 기본적으로 데이터를 여러 개로 분할하여 이용하는 것이다. 따라서 CV를 이용해서 여러번 실험하며 'Train data'만 잘 찾는 것이 아닌, 새로운 데이터도 잘 찾을 수 있는 모델을 만들 수 있다.


6. Hyper-Parameter 튜닝
 - 우리가 앞에서 다양한 모델들을 사용할 때, 모두 기본값을 사용했다. 하지만 다양한 하이퍼 파라미터를 설정하여 모델의 정확도를 바꿀 수 있다.
 - 모델을 튜닝하려면 실제로 모델을 이해해야 한다. 따라서 Decision Tree에 대해서 더 알아보자.
 
 (1) Decision Tree
  1) 장점
   - 이해하고, 해석하기 쉽다. Tree를 시각화 할 수 있다.
   - 데이터 전처리가 비교적 간단하다.
   - 트리를 사용하는 cost는 Tree를 학습시키는데 사용 된 데이터의 수에 Log를 한 것이다.
   - 수치적/범주적 데이터를 모두 처리할 수 있다. (다른 모델들은 일반적으로 하나의 유형 변수만을 이용해서 분석한다.)
   - 다중 출력 문제를 처리할 수 있다.
   - 조건에 대한 설명은 Boolean 논리로 쉽게 설명 가능하다. (결과 해석 용이하다!)
   - 통계를 통해서 모델을 검증할 수 있다. 따라서 모델의 신뢰성을 확인할 수 있다.
   - 데이터가 조금 이상하더라도 잘 수행된다.
   
  2) 단점
   - Decision Tree는 일반화 되지 않고 Train data에 지나치게 학습되어 복잡한 트리를 만들 수 있다. (Overfitting)
     이러한 문제를 해결하기 위해 프루닝, 리프 노드에 필요한 최소 샘플 수 설정, 트리 최대 깊이 설정 등의 방법이 필요하다.
   - Decision Tree는 데이터의 작은 차이로 완전히 다른 트리가 생성될 수 있고, 완전히 다른 결과를 가져올 수 있다.
     다양한 Decision Tree를 앙상블하여 해결할 수 있다.
   - 휴리스틱 알고리즘을 기반으로 한다. (최적을 반환하지는 못하다.)
     앙상블을 통해 여러 트리를 학습시키므로 해결할 수 있다.
   - Decision Tree는 XOR, Multi plexer 등의 문제를 쉽게 표현할 수 없다.
   - 일부 클래스가 지배적인 경우 의사 결정 트리는 편향된 트리를 만든다.
     따라서 Decision Tree를 적용하기 전에 데이터 집합의 균형을 유지해야 한다. (Invariance)

 (2) Decision Tree의 Hyper-Parameter
  - 우리는 다양한 Hyper-Parameter들을 적용해 보며, 각 적용한 모델들을 ParameterGrid/GridSearchCV/Sklearn Score 등을 이용해 평가하며 조정해 갈 것이다. 또한 Graphviz를 이용하여 시각화 하고, ROC_AUC등의 평가 방법도 이용할 것이다.  
 class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)
 
 (3) Decision Tree의 Hyper-Parameter 튜닝 시작!
  1) Base Model (튜닝 전 모델 확인)
dtree = tree.DecisionTreeClassifier(random_state = 0) #튜닝 전 모델
base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)
dtree.fit(data1[data1_x_bin], data1[Target]) #튜닝 전 모델 학습

print('BEFORE DT Parameters: ', dtree.get_params()) #튜닝 전 파라미터 출력
#튜닝 전 스코어 출력
print("BEFORE DT Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE DT Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE DT Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
#print("BEFORE DT Test w/bin set score min: {:.2f}". format(base_results['test_score'].min()*100))
print('-'*10)

  2) 모델 튜닝
#모델 파라미터 튜닝시키기
param_grid = {'criterion': ['gini', 'entropy'],  #Scoring 방법을 정의한다. Information gain을 계산하기 위한 2가지 방법이 있다. Default는 gini이다.
              #'splitter': ['best', 'random'], #Split 방법을 정의한다. 2가지 방법이 있는데 Default는 best이다.
              'max_depth': [2,4,6,8,10,None], #max depth 트리는 깊이가 커질 수 있다. Default는 none이다.
              #'min_samples_split': [2,5,10,.03,.05], #split을 하기 전 minimum subset 크기를 설정한다. Default는 2이다.
              #'min_samples_leaf': [1,5,10,.03,.05], #split을 한 이후 minimum subset 크기를 설정한다. Default는 1이다.
              #'max_features': [None, 'auto'], #split을 할 때 고려해야 할 feature의 최대 크기를 정한다. Default는 none 혹은 all이다.
              'random_state': [0] #Seed 또는 난수 생성기이다.
             }
#print(list(model_selection.ParameterGrid(param_grid)))

#튜닝된 모델로 학습시키기
tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)
tune_model.fit(data1[data1_x_bin], data1[Target]) #튜닝된 모델로 학습을 시킨다.

  3) 튜닝 된 모델 평가
print('AFTER DT Parameters: ', tune_model.best_params_) #파라미터 확인
#모델 평가
print("AFTER DT Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER DT Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER DT Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
print('-'*10)


7. Feature Selection을 통해 모델 튜닝하기
 - 모델은 독립변수(input)에 따른 종속변수(output)로 학습을 한다.
 - 독립 변수가 많다고 꼭 더 좋은 모델을 만드는 것은 아니다. '올바른 독립 변수'를 설정하는 게 중요하다!
 - Feature를 잘 선택해야 좋은 모델을 만들 수 있다.
   Cross Validation과 RFE를 사용해서 선택할 수 있다.

 (1) Feature Selection 하기
  1) 기본 모델
# Feature Selection을 하기 전 기본 모델의 형태, 스코어를 확인한다.
print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) 
print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)

print("BEFORE DT RFE Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE DT RFE Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
print('-'*10)

  2) Feature Selection 하기
dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split) #RFE, CV

#Feature Selection 한 모델로 학습시키기
dtree_rfe.fit(data1[data1_x_bin], data1[Target])
X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()] #줄어든 Feature를 다시 X로 이용!
rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)

  3) Feature Selection 적용 모델 평가
#모델의 형태
print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) 
print('AFTER DT RFE Training Columns New: ', X_rfe)

#모델 평가
print("AFTER DT RFE Training w/bin score mean: {:.2f}". format(rfe_results['train_score'].mean()*100)) 
print("AFTER DT RFE Test w/bin score mean: {:.2f}". format(rfe_results['test_score'].mean()*100))
print("AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}". format(rfe_results['test_score'].std()*100*3))
print('-'*10)

  4) rfe 모델 튜닝하기
#모델 튜닝
rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)

#튜닝한 모델로 학습시키기
rfe_tune_model.fit(data1[X_rfe], data1[Target])

#튜닝한 모델 스코어 확인
print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)
print("AFTER DT RFE Tuned Training w/bin score mean: {:.2f}". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER DT RFE Tuned Test w/bin score mean: {:.2f}". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
print('-'*10)

 (2) 그래프로 시각화 해보기
import graphviz 
dot_data = tree.export_graphviz(dtree, out_file=None, 
                                feature_names = data1_x_bin, class_names = True,
                                filled = True, rounded = True)
graph = graphviz.Source(dot_data) 
graph


8. Validate and Implement
 (1) 앙상블 하기
  - 모델을 구성할 때 꼭 하나의 모델만을 사용할 필요는 없다.
  - 여러 모델들이 출력한 결과를 voting 하게 해서, voting 한 결과값을 이용할 수 있다.
  # 앙상블 voting
  vote_est = [
    #다양한 모델들을 불러와서 앙상블한다.
    ('ada', ensemble.AdaBoostClassifier()),
    ('bc', ensemble.BaggingClassifier()),
    ('etc',ensemble.ExtraTreesClassifier()),
    ('gbc', ensemble.GradientBoostingClassifier()),
    ('rfc', ensemble.RandomForestClassifier()),

    ('gpc', gaussian_process.GaussianProcessClassifier()),
    
    ('lr', linear_model.LogisticRegressionCV()),
    
    ('bnb', naive_bayes.BernoulliNB()),
    ('gnb', naive_bayes.GaussianNB()),
    
    ('knn', neighbors.KNeighborsClassifier()),
    
    ('svc', svm.SVC(probability=True)),
    
    ('xgb', XGBClassifier())
]

 (2) Voting 룰 정하기
  - Hard Vote / Soft Vote 등 각 모델들이 Voting을 할 때의 룰을 정할 수 있다.
  1) Hard Vote or majority rule
# 앞에서 구축해놨던 앙상블 모델인 vote_est로, 'hard'의 voting 방법을 통해 모델을 구축한다.
vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard') #앙상블 모델을 hard한 방법으로 vote!
vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split) #CV
vote_hard.fit(data1[data1_x_bin], data1[Target]) #학습시키기

# Hard voting 평가
print("Hard Voting Training w/bin score mean: {:.2f}". format(vote_hard_cv['train_score'].mean()*100)) 
print("Hard Voting Test w/bin score mean: {:.2f}". format(vote_hard_cv['test_score'].mean()*100))
print("Hard Voting Test w/bin score 3*std: +/- {:.2f}". format(vote_hard_cv['test_score'].std()*100*3))
print('-'*10)

  2) Soft Vote or Weighted Probabilities
# 앞에서 구축해놨던 앙상블 모델인 vote_est로, 'soft'의 voting 방법을 통해 모델을 구축한다.
vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft') #앙상블 모델을 soft한 방법으로 vote!
vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split) #CV
vote_soft.fit(data1[data1_x_bin], data1[Target]) #학습시키기

# Soft voting 평가
print("Soft Voting Training w/bin score mean: {:.2f}". format(vote_soft_cv['train_score'].mean()*100)) 
print("Soft Voting Test w/bin score mean: {:.2f}". format(vote_soft_cv['test_score'].mean()*100))
print("Soft Voting Test w/bin score 3*std: +/- {:.2f}". format(vote_soft_cv['test_score'].std()*100*3))
print('-'*10)

 (3) 각 앙상블 속의 모델들 하이퍼파라미터 튜닝하기
  - grid Search CV를 통해 하이퍼파라미터를 튜닝하자.
  1) grid Search CV 설정  
grid_n_estimator = [10, 50, 100, 300]
grid_ratio = [.1, .25, .5, .75, 1.0]
grid_learn = [.01, .03, .05, .1, .25]
grid_max_depth = [2, 4, 6, 8, 10, None]
grid_min_samples = [5, 10, .03, .05, .10]
grid_criterion = ['gini', 'entropy']
grid_bool = [True, False]
grid_seed = [0]

  2) 모델별 하이퍼파라미터 튜닝하기
  grid_param = [ #grid를 통해 하이퍼 파라미터 튜닝!
            [{
	    #AdaBoostClassifier 튜닝
            'n_estimators': grid_n_estimator, #default=50
            'learning_rate': grid_learn, #default=1
            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R
            'random_state': grid_seed
            }],
           
            [{
	    #BaggingClassifier 튜닝
            'n_estimators': grid_n_estimator, #default=10
            'max_samples': grid_ratio, #default=1.0
            'random_state': grid_seed
             }],
	     
	     [{
	     #ExtraTreesClassifier 튜닝
            'n_estimators': grid_n_estimator, #default=10
            'criterion': grid_criterion, #default=”gini”
            'max_depth': grid_max_depth, #default=None
            'random_state': grid_seed
             }],

            [{
	    #GradientBoostingClassifier 튜닝
            #'loss': ['deviance', 'exponential'], #default=’deviance’
            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.
            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.
            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”
            'max_depth': grid_max_depth, #default=3   
            'random_state': grid_seed
             }],
	     
	     [{
	     #RandomForestClassifier 튜닝
            'n_estimators': grid_n_estimator, #default=10
            'criterion': grid_criterion, #default=”gini”
            'max_depth': grid_max_depth, #default=None
            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.
            'random_state': grid_seed
             }],
    
            [{    
	    #GaussianProcessClassifier 튜닝
            'max_iter_predict': grid_n_estimator, #default: 100
            'random_state': grid_seed
            }],
	    
	    [{
	    #LogisticRegressionCV 튜닝
            'fit_intercept': grid_bool, #default: True
            #'penalty': ['l1','l2'],
            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs
            'random_state': grid_seed
             }],
                
            [{
	    #BernoulliNB 튜닝
            'alpha': grid_ratio, #default: 1.0
             }],
        
            #GaussianNB - 
            [{}],
	    
	    [{
	    #KNeighborsClassifier 튜닝
            'n_neighbors': [1,2,3,4,5,6,7], #default: 5
            'weights': ['uniform', 'distance'], #default = ‘uniform’
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
            }],
            
            [{
	    #SVC 튜닝
            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
            'C': [1,2,3,4,5], #default=1.0
            'gamma': grid_ratio, #edfault: auto
            'decision_function_shape': ['ovo', 'ovr'], #default:ovr
            'probability': [True],
            'random_state': grid_seed
             }],
	     
	     [{
	     #XGBClassifier 튜닝
            'learning_rate': grid_learn, #default: .3
            'max_depth': [1,2,4,6,8,10], #default 2
            'n_estimators': grid_n_estimator, 
            'seed': grid_seed  
             }]   
        ]

  3) 모델별로 튜닝한 파라미터를 각 모델에 직접 적용하기.
  #Time Count
  start_total = time.perf_counter()
  
  #모델별로 튜닝한 파라미터를 각 모델에 직접 적용!
  for clf, param in zip (vote_est, grid_param):
    
    start = time.perf_counter()        
    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')
    best_search.fit(data1[data1_x_bin], data1[Target])
    run = time.perf_counter() - start

    best_param = best_search.best_params_
    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))
    clf[1].set_params(**best_param) 
    
    run_total = time.perf_counter() - start_total
    print('Total optimization time was {:.2f} minutes.'.format(run_total/60))

    print('-'*10)
    
  4) Voting rule 정하기
   - Hard Vote or majority rules
grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')
grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)
grid_hard.fit(data1[data1_x_bin], data1[Target])

print("Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}". format(grid_hard_cv['train_score'].mean()*100)) 
print("Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}". format(grid_hard_cv['test_score'].mean()*100))
print("Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}". format(grid_hard_cv['test_score'].std()*100*3))
print('-'*10)

   - Soft Vote or weighted probabilities
grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')
grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)
grid_soft.fit(data1[data1_x_bin], data1[Target])

print("Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}". format(grid_soft_cv['train_score'].mean()*100)) 
print("Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}". format(grid_soft_cv['test_score'].mean()*100))
print("Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}". format(grid_soft_cv['test_score'].std()*100*3))
print('-'*10)


9. Submission 파일 만들기!
 (1) 모델링 준비
print(data_val.info())
print("-"*10)

 (2) Handmade Decision Tree 스코어 확인
  - data_val의 Survived에 Handmade Decision Tree 모델이 predict한 결과를 저장한다.
data_val['Survived'] = mytree(data_val).astype(int)
# 스코어 확인하는 코드

 (3) Hard vote / grid 적용해서 앙상블한 스코어 확인
  - data_val의 Survived에 Hard vote / grid를 적용해서 앙상블한 모델이 predict한 결과를 저장한다.
  - (생략) 앞에서 만든 여러 모델들의 스코어를 다 확인한 결과, 이 모델의 스코어가 가장 높았다! 이 Predict를 이용해서 Submission 하자.
data_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])
# 스코어 확인하는 코드

 (4) Submission 파일 생성
  - data_val에 저장한 predict 결과를 Submission 파일로 생성한다.
submit = data_val[['PassengerId','Survived']] #Columns 생성
submit.to_csv("../working/submit.csv", index=False) #Submit 파일 저장

#data_val에 저장한 predict 결과를 Submission 파일에 작성한다.
print('Validation Data Distribution: \n', data_val['Survived'].value_counts(normalize = True))
submit.sample(10) #샘플 10개 확인
