A Data Science Framework To Achieve 99% Accuracy
 - 발표자 : 서기원
 - 발표 일자 : 2019. 05. 15
 - Original address : https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy
 - 내용 :


1. How a Data Scientist Beat the Odds
 - 컴페티션 결과 : 바이너리 예측(발생 or 발생하지 않음)


2. A Data Science Framework
 (1) 문제 정의
  - 알고리즘 등의 설계를 시작하기 전, 문제 정의를 해야 한다.

 (2) 데이터 수집

 (3) 데이터 가공
  - wild/managable한 데이터를 가공한다.
  - 데이터 아키텍쳐 구성, 데이터 추출, 데이터 정리(누락 및 이상 값 식별) 등의 과정

 (4) 탐색적 분석
  - 데이터들 사이의 패턴, 분류, 상관관계 등을 비교하기 위해 그래프를 그려 데이터를 탐색해 보는 것이 좋다.

 (5) 모델 데이터
  - 데이터 세트와 prediction을 생각하며 잘 분석하여 알고리즘을 선정한다.
  - 학습시킬 모델을 잘 선정하는 것이 중요하다!

 (6) 모델 구현 및 정확도 검사
  - 모델을 구현하여 학습시킨 후, 정확도를 검사한다.
  - Overfit / Generalize / Underfit 하는지 확인한다.

 (7) 최적화 및 Strategize
  - 모델을 최적화 시킨다!
  - 파라미터 조정 등의 미세조정 과정이다.


3. Step
 (1) Step 1 : Define the Problem
  - 문제 정의 : 이 프로젝트의 문제는 타이타닉 호의 승객 생존 결과를 예측하는 알고리즘을 개발하는 것이다.
  - 프로젝트 요약 : 타이타닉의 침몰은 역사상 가장 악명 높은 난파선 중 하나이다. 1912년 4월 15일 타이타닉은 빙산과 충돌한 후 침몰하여 2224명의 승객과
승무원 중 1502 명이 사망했다. 난파선으로 인해 많은 사람이 죽은 이유 중 하나는 승객과 승무원을 위한 구명정이 충분하지 않았기 때문이다. 침몰에서 살아남는
데는 몇 가지 행운이 있었지만 여성, 어린이 및 상류층과 같은 일부 그룹은 다른 사람들보다 생존 가능성이 더 컸다. 이러한 과제에서 우리는 어떤 종류의 사람들이
생존 할 가능성이 있는지에 대해 분석해야 한다! 기계학습 도구를 적용하여 해당 승객이 이 비극에서 살아남을지를 예측하자.
  - 기술 : 이진 분류, 파이썬/R

 (2) Step 2 : Gather the Data
  - 캐글에서 트레인/테스트 데이터 셋을 준다!

 (3) Step 3 : Prepare Data for Consumption
  1-1) Import Libraries
   - 코드는 Python 3.x로 작성 되었다.
   - 라이브러리는 필요한 작업을 수행하기 위해 미리 작성된 기능을 제공한다.
import sys #system parameters로 접근한다.
import pandas as pd #기계학습에 도움되는 판다스
import matplotlib #시각화에 도움됨
import numpy as np #수학적 계산에 도움 됨
import scipy as sp #계산에 필요한 함수들을 가지고 있음
import IPython
from IPython import display #data frame을 jupyter notebook에서 잘 출력하기 위함
import sklearn #많은 기계학습 알고리즘을 가지고 있음
import random
import time

#warnings 무시
import warnings
warnings.filterwarnings('ignore')
print('-'*25)

from subprocess import check_output
print(check_output(["Is", "../input"]).decode("utf8")) #data set들 확인하기. 캐글 커널상에서 돌릴 경우, ../input/을 통해 input data를 이용할 수 있다.

  1-2) Load Data Modelling Libraries
   - 기본적으로 scikit-learn 라이브러리를 사용하여 기계학습 알고리즘을 이용할 것이다.
   - 데이터 시각화를 위해 matplotlib와 seaborn 라이브러리를 사용한다.
#일반적인 Model Algorithms
from sklearn import svm, tree, linear_model, neighbors, naive_bays, ensemble, discriminant_analysis, gaussian_process
from xgboost import XGBClassifier

#일반적인 Model 도우미들
from sklearn.preprocessing import OneHotEncoder, LabelEncoder #데이터 전처리
from sklearn import feature_selection #특징 선택
from sklearn import model_selection #모델 선택
from sklearn import metrics #평가

#Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns #seaborn 시각화
from pandas.tools.plotting import scatter_matrix #산점도

#Visualization 이용시 필수 함수
%matplotlib inline #주피터 노트북에서 mlt를 볼 수 있게 해줌
mpl.style.use('ggplot') #mlt 스타일 지정
sns.set_style('white') #seaborn 스타일 지정
pylab.rcParms['figure.gifsize'] = 12, 8 #figsize

  2-1) Meet and Greet Data
   - 데이터 유형/값, 변수 등 데이터를 탐색해 본다.
   - info()와 sample() 함수로 데이터 유형의 간결한 개요를 볼 수 있다.
   - Survived : 생존한 경우 1, 생존 못 한 경우 0
     PassengerID / Ticket : 결과 변수에 영향을 주지 않고 '대상을 식별하기 위한 것'으로 가정한다. 분석에서 제외된다.
     Pclass : 상위 클래스는 1, 중간 클래스는 2, 하위 클래스는 3이다.
     Name : 공칭 데이터 유형. 성의 성별 및 의사/석사 등을 구분할 수 있게 도와준다.
     Sex / Embarked : 공칭 데이터 유형. 수학 계산을 위해 더미 변수로 변환되어야 한다.
     Age / Fare : 양적 데이터 변수이다.
     SibSP / Parch : SibSP는 관련된 형제 / 배우자의 수를 나타내며, Parch는 관련된 부모 / 자녀의 수를 나타낸다. 개별의 정량 데이터 유형이다.
     Cabin : 사고가 발생했을 때 선박의 대략적인 위치와 갑판 수준의 SES에서 엔지니어링에 사용할 수 있는 공칭 데이터 유형이다. null이 많으므로 분석에서 제외된다.
data_raw = pd.read_csv('../input/train.csv') #판다스를 이용해 input인 train 데이터의 csv파일을 읽는다. train을 train/test로 쪼개고 또 각각 input/output 피쳐를 구분해서 이용할 예정!
data_val = pd.read_csv('../input/test.csv') #test에 대한 output을 최종적으로 예측해야 한다!
data1 = data.raw.copy(deep = True) #데이터를 잘 이용하기 위해 복제!
data_cleaner = [data1, data_val] #cleaner를 이용해서 간단하게 데이터 한 번 정제하기.

print(data_raw.info()) #data_raw 데이터에 대한 information을 간단하게 확인한다.
data_raw.sample(10)

  2-2) The 4 C's Data Cleaning : Correcting, Completing, Creating, and Converting
   - 이 단계에서는 비정상적 값과 특이 치를 수정하고, 누락 정보를 완성하며, 분석을 위해 새 기능을 만들고, 계산 및 표현을 위해 형식을 변환한다.
   - Correcting : 비정상적의 데이터를 수정한다. 비정상적의 예는 80세가 아닌 800세인 경우이다.
     Completing : Null / Nan 값 등 값이 누락 된 경우 모델이 처리 못할 수 있다. 이 문제 해결하기! Nan을 삭제하거나 0, 평균 값 취하기 등 다양한 방법이 있다.
     Creating : 피쳐 별로 생존에 영향을 줬는지 title을 부여할 것이다.
     Converting : 다양한 데이터들을 계산 가능한 데이터로 변환한다!
print('Train columns with null values : \n', data1.isnull().sum()) #Train 데이터 중 null을 확인한다.
print("-"*10)

print('Test/Validation columns with null vaues : \n', data_val.isnull().sum()) #Test/Validation 데이터 중 null을 확인한다.
print("-"*10)

data_raw.describe(include = 'all') #data_raw의 데이터를 확인한다.

  2-3) Clean Data
#####Completing#####
for dataset in data_cleaner : #data_cleaner 함수 지정
	dataset['Age'].fillna(dataset['Age'].median(), inplace = True) #Age 중 빈 영역을 평균값으로 채운다.
	dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True) #Embarked 중 빈 부분을 mode로 채운다.
	dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True) #Fare 중 빈 부분을 평균값으로 채운다.

drop_column = ['PassengerId', 'Cabin', 'Ticket'] #결과에 영향을 주지 않는다고 판단되는 피쳐를 train set에서 제거한다.
data1.drop(drop_column, axis=1, inplace = True)

print(data1.isnull().sum()) #data1의 null 확인
print("-"*10)
print(data_val.isnull().sum()) #data_val의 null 확인
##########

#####Create#####
for dataset in dat_cleaner:
	dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 #형제/배우자 등을 포함해 Family Size라는 새로운 특징으로 만듦.
	dataset['IsAlone'] = 1 #혼자일 경우 1
	dataset['IsAlone'].loc[dataset['FamilySize']>1] = 0 #가족 구성원이 1 초과일 경우 0
	dataset['Title'] = dataset['Name'].str.split(", ", expand = True)[1].str.split(".", expand = True)[0] #',' 혹은 '.'으로 이름을 분리하고, Title(Miss/mr 등)을 얻는다.
	dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)
	dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)

	#Title 정제(Mr/Miss/Mrs/Master/Misc)
	stat_min = 10
	title_names = (data1['Title'].value_counts < stat_min) #10보다 작은 경우
	data1['Title'] = data1['Title'].apply(lambda x:"Misc" if title_names.loc[x] == True else x)
	print(data1['Title'].value_counts())
	print("-"*10)

	#데이터 확인
	data1.info()
	data_val.info()
	data1.sample(10)
##########

  2-4) Convert Formats
   - sklearn / pandas를 이용하여 범주형 데이터를 수학적으로 변환할 것이다.
   - 또한 데이터를 모델링에 이용하기 위해 x(input)와 y(output)로 나눌 것이다.
#####Convert#####
#수학적 계산을 위해 정제시킨 데이터들을 Encoder를 이용해 카테고리화 한다.
label = LabelEncoder()
for dataset in dat_cleaner :
	dataset['Sex_Code'] = label.fit_transform(datset['Sex'])
	dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])
	dataset['Title_Code'] = label.fit_transform(dataset['Title'])
	dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])
	dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])

# x(input)에 따른 y(output)을 정의한다.
Target = ['Survived']

# x(input)을 정의한다.
data1_x = ['Sex', Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #정제되기 전의 x
data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #정제되어 계산 가능한 x
data1_xy = Target + dadta1_x
print('Original X Y: ', data1_xy, '\n')

# binary 하게 만든 x
data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']
data1_xy_bin = Target + data1_x_bin
print('Bin X Y: ', data1_xy_bin, '\n')

# 더미 피처에 대한 x 및 y 변수 정의 original
data1_dummy = pd.get_dummies(data1[data1_x])
data1_x_dummy = data1_dummy.columns.tolist()
data1_xy_dummy = Target + data1_x_dummy
print('Dummy X Y: ', data1_xy_dummy, '\n')

data1_dummy.head()
##########

  2-5) Double Check Cleaned Data
   - 데이터를 전부 정제 했는지 더블 체크한다.
print('Train columns with null values: \n', data1.isnull().sum())
print("-"*10)
print (data1.info())
print("-"*10)

print('Test/Validation columns with null values: \n', data_val.isnull().sum())
print("-"*10)
print (data_val.info())
print("-"*10)

data_raw.describe(include = 'all')

  2-6) Split Training and Testing Data
   - Train set을 Train과 Test 데이터 셋으로 분리한다. 0.75 : 0.25의 비율로 분리한다.
   - sklearn의 train_test_split 함수를 사용한다.
train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)
train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)
train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)

print("Data1 Shape: {}".format(data1.shape))
print("Train1 Shape: {}".format(train1_x.shape))
print("Test1 Shape: {}".format(test1_x.shape))

train1_x_bin.head()

 (4) Step 4 : Perform Exploratory Anaysis with Statistics
  - 데이터 정리는 완료 되었다. 데이터를 탐색하기 위해 통계/그래프를 이용한다. 데이터의 기능을 분류하고 대상 변수 및 서로의 상관관계를 분석할 수 있다.
#각 피쳐인 Column들의 값의 평균과 Survival의 상관관계를 알아본다.
  1-1) 데이터 상관관계 확인
for x in data1_x:
    if data1[x].dtype != 'float64' :
        print('Survival Correlation by:', x)
        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean()) #Target(Survival)과 각 Column의 평균값을 groupby하여 상관관계를 알아본다.
        print('-'*10, '\n')

print(pd.crosstab(data1['Title'],data1[Target[0]]))

  1-2) 그래프로 분포 및 상관관계 확인
#양적 데이터의 Graph 분포
plt.figure(figsize=[16,12])

#Fare 분포
plt.subplot(231)
plt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)
plt.title('Fare Boxplot')
plt.ylabel('Fare ($)')

#Age 분포
plt.subplot(232)
plt.boxplot(data1['Age'], showmeans = True, meanline = True)
plt.title('Age Boxplot')
plt.ylabel('Age (Years)')

#FamilySize 분포
plt.subplot(233)
plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)
plt.title('Family Size Boxplot')
plt.ylabel('Family Size (#)')

#Fare과 Survived 상관관계
plt.subplot(234)
plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], 
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Fare Histogram by Survival')
plt.xlabel('Fare ($)')
plt.ylabel('# of Passengers')
plt.legend()

#Age와 Survived 상관관계
plt.subplot(235)
plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], 
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Age Histogram by Survival')
plt.xlabel('Age (Years)')
plt.ylabel('# of Passengers')
plt.legend()

#FimilySize와 Survived 상관관계
plt.subplot(236)
plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], 
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Family Size Histogram by Survival')
plt.xlabel('Family Size (#)')
plt.ylabel('# of Passengers')
plt.legend()

  1-3) Seaborn 그래프
fig, saxis = plt.subplots(2, 3,figsize=(16,12))

#barplot형 그래프
sns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0]) #Embarked와 Survived 상관관계
sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1]) #Pclass와 Survived 상관관계
sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2]) #IsAlone과 Survived 상관관계

#pointplot형 그래프
sns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0]) #FareBin과 Survived 상관관계
sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1]) #AgeBin과 Survived 상관관계
sns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2]) #FamilySize와 Survived 상관관계

#피쳐끼리의 상관관계 중요도 비교
fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))

#Pclass vs Fare
sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)
axis1.set_title('Pclass vs Fare Survival Comparison')

#Pclass vs Age
sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)
axis2.set_title('Pclass vs Age Survival Comparison')

#Pclass vs Family Size
sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)
axis3.set_title('Pclass vs Family Size Survival Comparison')

fig, qaxis = plt.subplots(1,3,figsize=(14,12))

#Sex vs Embarked
sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])
axis1.set_title('Sex vs Embarked Survival Comparison')

#Sex vs Pclass
sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])
axis1.set_title('Sex vs Pclass Survival Comparison')

#Sex vs IsAlone
sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])
axis1.set_title('Sex vs IsAlone Survival Comparison')

fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))

#family size vs sex in Survived
sns.pointplot(x="FamilySize", y="Survived", hue="Sex", data=data1,
              palette={"male": "blue", "female": "pink"},
              markers=["*", "o"], linestyles=["-", "--"], ax = maxis1)

#Pclass vs Sex in Survived
sns.pointplot(x="Pclass", y="Survived", hue="Sex", data=data1,
              palette={"male": "blue", "female": "pink"},
              markers=["*", "o"], linestyles=["-", "--"], ax = maxis2)

#Pclass vs Sex vs Embarked in Survived
e = sns.FacetGrid(data1, col = 'Embarked')
e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')
e.add_legend()

#Age에 따라 살아남는가, 살아남지 못하는가 확인
a = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )
a.map(sns.kdeplot, 'Age', shade= True )
a.set(xlim=(0 , data1['Age'].max()))
a.add_legend()

#Sex vs Class vs Age in Survived
h = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')
h.map(plt.hist, 'Age', alpha = .75)
h.add_legend()

#전체 데이터 셋의 pair 그래프
pp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )
pp.set(xticklabels=[])

  1-4) 히트맵
def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(14, 12))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)
    
    _ = sns.heatmap(
        df.corr(), 
        cmap = colormap,
        square=True, 
        cbar_kws={'shrink':.9 }, 
        ax=ax,
        annot=True, 
        linewidths=0.1,vmax=1.0, linecolor='white',
        annot_kws={'fontsize':12 }
    )
    
    plt.title('Pearson Correlation of Features', y=1.05, size=15)

correlation_heatmap(data1)

 (5) Step 5 : Model Data
  - 정답을 포함하는 Supervised Learning 방법을 이용한다.
  - 분류하는 알고리즘이 필요하다.
  - 문제에 맞는 최상의 알고리즘을 선택하는 방법은?
    그런 것은 없다! 여러 알고리즘을 시도하고, 조정하고, 비교해봐야 한다.
#머신러닝 알고리즘들!
MLA = [
    #Ensemble Methods
    ensemble.AdaBoostClassifier(),
    ensemble.BaggingClassifier(),
    ensemble.ExtraTreesClassifier(),
    ensemble.GradientBoostingClassifier(),
    ensemble.RandomForestClassifier(),

    #Gaussian Processes
    gaussian_process.GaussianProcessClassifier(),
    
    #GLM
    linear_model.LogisticRegressionCV(),
    linear_model.PassiveAggressiveClassifier(),
    linear_model.RidgeClassifierCV(),
    linear_model.SGDClassifier(),
    linear_model.Perceptron(),
    
    #Navies Bayes
    naive_bayes.BernoulliNB(),
    naive_bayes.GaussianNB(),
    
    #Nearest Neighbor
    neighbors.KNeighborsClassifier(),
    
    #SVM
    svm.SVC(probability=True),
    svm.NuSVC(probability=True),
    svm.LinearSVC(),
    
    #Trees    
    tree.DecisionTreeClassifier(),
    tree.ExtraTreeClassifier(),
    
    #Discriminant Analysis
    discriminant_analysis.LinearDiscriminantAnalysis(),
    discriminant_analysis.QuadraticDiscriminantAnalysis(),
    
    #xgboost
    XGBClassifier()    
    ]

#dataset을 cross-validation으로 분할하기.
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) #test : train = 3 : 6

#MLA 알고리즘들을 비교, 평가하기 위한 배열
MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']
MLA_compare = pd.DataFrame(columns = MLA_columns)

#MLA 알고리즘들의 predictions
MLA_predict = data1[Target]

#MLA의 수행 결과를 테이블 형식으로 저장하기
row_index = 0 #행
for alg in MLA:

    #name 및 파라미터 설정
    MLA_name = alg.__class__.__name__
    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name
    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())
    
    #cross validation을 통한 모델 평가
    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)

    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean() #학습 시간
    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() #학습 정확도
    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() #테스트 정확도
    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3 #일어날 수 있는 최악의 결과
    
    #MLA prediction 저장
    alg.fit(data1[data1_x_bin], data1[Target])
    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])
    
    row_index+=1 #for문 돌아가며 행 추가

MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)
MLA_compare #MLA 알고리즘들의 비교 테이블 출력!

#그래프를 통해서 각 알고리즘들의 성능 비교해 보기
sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')
plt.title('Machine Learning Algorithm Accuracy Score \n')
plt.xlabel('Accuracy Score (%)')
plt.ylabel('Algorithm')


4. 모델 평가
 - 기본 MLA(기계 학습 알고리즘들)을 이용해서 82%의 정확도를 기록했다.
 - 지금까지 정리해보면 다음과 같다. 기계 학습 알고리즘을 생각할 때 다음의 조건을 생각하며 각 항목에서 Class 별로 생존/사망을 정하는 게임이라고 생각하면 좋다.
 - 남성 (81%) 사망 / 여성 (74%) 생존
   1등급 (97%) 생존 / 2등급 (92%) 생존
   S항 (63%) 사망